{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a85e5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "try:\n",
    "    __import__(\"lightning\")\n",
    "except ImportError:\n",
    "    pip.main([\"install\", \"lightning\"])\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a3e1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning as L\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8c3bb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca73e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab={\n",
    "    '<SOS>': 0,\n",
    "    'lets':1,\n",
    "    'to':2,\n",
    "    'go':3,\n",
    "}\n",
    "output_vocab={\n",
    "    '<SOS>': 0,\n",
    "    'ir':1,\n",
    "    'vamos':2,\n",
    "    'y':3,\n",
    "    '<EOS>': 4,\n",
    "}\n",
    "inputs=torch.tensor([[1,3],[2,3]])\n",
    "labels=torch.tensor([[2],[1]])\n",
    "\n",
    "dataset=TensorDataset(inputs, labels)\n",
    "dataloader=DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "124889ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        # Initialize the positional encoding module.\n",
    "        # d_model: Dimension of the model.\n",
    "        # max_len: Maximum length of the input sequence.\n",
    "        # This module generates positional encodings for input sequences.\n",
    "        # It uses sine and cosine functions to encode the positions of tokens in the sequence.\n",
    "        # The positional encodings are added to the input embeddings to provide information about the position of\n",
    "        # each token in the sequence.\n",
    "        # The positional encodings are computed using sine and cosine functions of different frequencies.\n",
    "        # The positional encodings are stored in a tensor of shape (max_len, d_model).\n",
    "        # The positional encodings are computed using the formula:\n",
    "        # PE(pos, 2i) = sin(pos / (10000^(2i / d_model)))\n",
    "        # PE(pos, 2i+1) = cos(pos / (10000^(2i / d_model)))\n",
    "        # where pos is the position of the token in the sequence, i is the dimension index\n",
    "        # and d_model is the dimension of the model.\n",
    "        super().__init__()\n",
    "        pe=torch.zeros(max_len, d_model)\n",
    "        # Create a tensor to hold the positional encodings. 0,1,2 # ... max_len-1 are the positions in the sequence.\n",
    "        # The tensor is initialized to zeros with shape (max_len, d_model).\n",
    "        # each index will represent the position of a token in the sequence.\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        # So for each position we will have two values, one for sine and one for cosine. This will be extended based on the d_model.\n",
    "        # The position tensor is created using torch.arange, which generates a sequence of numbers from 0 to max_len-1.\n",
    "        # The unsqueeze(1) operation adds a new dimension to the tensor, making it a column vector.\n",
    "        # This is necessary for broadcasting when computing the sine and cosine functions.\n",
    "        # The position tensor will have shape (max_len, 1), where each row corresponds to a position in the sequence.\n",
    "        # To simply the computation and for the sake of clarity we compute the div term first: \n",
    "        # 1/ (10000^(2i / d_model))\n",
    "        div_term=1/torch.tensor(10000**(2*torch.arange(start=0, end=d_model, step=2).float()/d_model))\n",
    "\n",
    "        # Now we can compute the sine and cosine functions for each position and dimension.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the positional encoding module.\n",
    "        # x: Input tensor of shape (batch_size, seq_len, d_model).\n",
    "        # Returns the input tensor with positional encodings added.\n",
    "        # The positional encodings are added to the input embeddings to provide information about the position of\n",
    "        # each token in the sequence.\n",
    "        # The input tensor x is expected to have shape (batch_size, seq_len, d_model).\n",
    "        # The positional encodings are added to the input tensor along the last dimension (d_model).\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934101d",
   "metadata": {},
   "source": [
    "Take a step to rethink why we do 0::2 or 1::2 above...1,2,3\n",
    "position is position of the input sequence which has a length of 3, with 0,1,2 indices\n",
    "d_model is 2, 0,1 indices\n",
    "all rows from even columns for sin\n",
    "all rows odd columns for cos\n",
    "This gives us 3 div_terms, which we use for:\n",
    "\n",
    "```\n",
    "dim 0 → sin(... / div_term[0])\n",
    "\n",
    "dim 1 → cos(... / div_term[0])\n",
    "\n",
    "dim 2 → sin(... / div_term[1])\n",
    "\n",
    "dim 3 → cos(... / div_term[1])\n",
    "\n",
    "dim 4 → sin(... / div_term[2])\n",
    "\n",
    "dim 5 → cos(... / div_term[2])\n",
    "```\n",
    "See how div_term[i] is used for both dim 2i and 2i+1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f9f17b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3694377/212316298.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  div_term=1/torch.tensor(10000**(torch.arange(start=0, end=d_model, step=2).float()/d_model))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 3\n",
    "d_model = 2\n",
    "div_term=1/torch.tensor(10000**(torch.arange(start=0, end=d_model, step=2).float()/d_model))\n",
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7a21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c22dc4dc",
   "metadata": {},
   "source": [
    "### Now we will code attention\n",
    "Recap on Attention is availble in seq2seq wit attention code\n",
    "\n",
    "We will code:\n",
    "* Self Attention\n",
    "* Masked Self Attention\n",
    "* Encoder Decoder Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2e8be",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "Self attention allows us to find relationship of every word with every other word in the sequence/phrase including the word for which we are calculating attention with itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40fb52",
   "metadata": {},
   "source": [
    "PE_values.Q_w=Q\n",
    "\n",
    "PE_values.K_w=K\n",
    "\n",
    "PE_values.V_w=V\n",
    "\n",
    "Then we multiply Q with K to get similarity of each word in query iwth the keys for all of the words\n",
    "\n",
    "Then we normalize the similarities matrix normaly by /sqrt(2)\n",
    "\n",
    "Scaled Similarities=Q.KT=Similarities/sqrt(2)\n",
    "\n",
    "Apply softmax get scaled similarities \n",
    "\n",
    "then multiply these matrix with V to get attention scores by multiplies percentages in similarity matrixes with V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "238c0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.q_w= nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_w= nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_w= nn.Linear(d_model, d_model, bias=False)\n",
    "        self.row_dim=0\n",
    "        self.col_dim=1\n",
    "    def forward(self, encodings_for_q,encodings_for_k, encodings_for_v,mask=None):\n",
    "        # For normal self attention encodings_for_q, encodings_for_k and encodings_for_v are same.\n",
    "        # For encoder-decoder attention encodings_for_q are from decoder and encodings_for_k and encodings_for_v are from encoder.\n",
    "        q=self.q_w(encodings_for_q)\n",
    "        k=self.k_w(encodings_for_k)\n",
    "        v=self.v_w(encodings_for_v)\n",
    "\n",
    "        # calculate similarity scores\n",
    "        #  # q is the query, k is the key, v is the value.\n",
    "        # q*k^T/sqrt(d_model)\n",
    "        ## NOTE: It seems most people use \"reverse indexing\" for the dimensions when transposing k\n",
    "        ##       k.transpose(dim0, dim1) will transpose k by swapping dim0 and dim1\n",
    "        ##       In standard matrix notation, we would want to swap rows (dim=0) with columns (dim=1)\n",
    "        ##       If we have 3 dimensions, because of batching, and the batch was the first dimension\n",
    "        ##       And thus dims are defined batch = 0, rows = 1, columns = 2\n",
    "        ##       then dim0=-2 = 3 - 2 = 1. dim1=-1 = 3 - 1 = 2.\n",
    "        ##       Alternatively, we could put the batches in dim 3, and thus, dim 0 would still be rows\n",
    "        ##       and dim 1 would still be columns. I'm not sure why batches are put in dim 0...\n",
    "        ##\n",
    "        ##       Likewise, the q.size(-1) uses negative indexing to reverse to the number of columns in the query\n",
    "        ##       which tells us d_model. Alternatively, we could ust q.size(2) if we have batches in the first\n",
    "        ##       dimension or q.size(1) if we have batches in the 3rd dimension.\n",
    "        ##\n",
    "        ##       Since there are a bunch of ways to index things, I think the best thing to do is use\n",
    "        ##       variables \"row_dim\" and \"col_dim\" instead of numbers...\n",
    "        sims=torch.matmul(q,k.transpose(self.row_dim, self.col_dim))\n",
    "        scaled_dims=sims/torch.tensor(q.size(self.col_dim)**0.5)\n",
    "        if mask is not None:\n",
    "            scaled_dims=scaled_dims.masked_fill(mask=mask, value=-1e9)\n",
    "        # scaled_dims is the similarity scores between q and k.\n",
    "        attention_percents=F.softmax(scaled_dims, dim=self.col_dim)\n",
    "        # attention_percents is the attention distribution over the keys.\n",
    "        attention_scores=torch.matmul(attention_percents, v)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081ff27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1e4afe",
   "metadata": {},
   "source": [
    "Encoder\n",
    "We need to combiine\n",
    "* PE\n",
    "* Self Attention\n",
    "* Residual Connections\n",
    "\n",
    "How \n",
    "Sim=PE * Self Attention\n",
    "Sim+ PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9cc29d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_tokens=4,d_model=2,max_len=3):\n",
    "        super().__init__()\n",
    "        L.seed_everything(42)\n",
    "        # To stack more encoders output of one encoder will be input to other encoder\n",
    "        self.we=nn.Embedding(num_tokens,embedding_dim=d_model)\n",
    "        self.pe=PositionalEncoding(d_model,max_len)\n",
    "        self.self_attention=SelfAttention(d_model)\n",
    "        # Notes from Statquest on multi head attention\n",
    "        # We can have that by initializing multiple attention objects\n",
    "        # This will result in multiple attention values\n",
    "        # FOr example\n",
    "        ##\n",
    "        ## self.self_attention_2 = Attention(d_model=d_model)\n",
    "        ## self.self_attention_3 = Attention(d_model=d_model)\n",
    "        ##\n",
    "        # We have d_model=2 then using 3 self attention values result will be 2*3=6 self attention values per token\n",
    "        # So we need to have a matrix of attention values and reduce those to d_model=2 dimensions\n",
    "        # Get a 3*2 matrix and output 2,1 vector by doing a  linear operation\n",
    "        # self.reduce_attention_dim=nn.Linear(in_features=(num_attention_heads*d_model),out_features=d_model)\n",
    "    def forward(self,token_ids):\n",
    "        embeddings=self.we(token_ids)\n",
    "        position_encodings=self.pe(embeddings)\n",
    "        self_attention=self.self_attention(position_encodings,position_encodings,position_encodings)\n",
    "        # For multi head attention here we will do attention calculation\n",
    "        # self_atten_val_2=self.self_attention_2(..)\n",
    "        # self_atten_val_3=self.self_attention_3(..)\n",
    "        # Lastly concatenating all the attention values using torch.concat()\n",
    "        # and running through reduction layer\n",
    "        # final_self_attention_layer=self.reduce_attention_dim(final_attention_values)\n",
    "        # Then lasting adding the attention to position_encodings and returning the final output\n",
    "        output_values=self_attention+position_encodings\n",
    "\n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de48ba1a",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "For a decoder to work we have to implement the following in order:\n",
    "* Positional Encoding\n",
    "* Self_attentoion (Masked)\n",
    "* Residul Connection\n",
    "* Encoder_decoder Attention\n",
    "* Fully Conencted Layer\n",
    "* Softmax- (CrossEntropyLoss as crossentropyloss implements the softmax for us)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb08314",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "704ecde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_tokens=4,d_model=2,max_len=3):\n",
    "        super().__init__()\n",
    "        self.we=nn.Embedding(num_embeddings=num_tokens,embedding_dim=d_model)\n",
    "        self.pe=PositionalEncoding(d_model=d_model,max_len=max_len)\n",
    "        self.self_attention=SelfAttention(d_model=d_model)\n",
    "        self.enc_dec_attention=SelfAttention(d_model=d_model)\n",
    "        self.fc_layer=nn.Linear(in_features=d_model,out_features=num_tokens)\n",
    "        self.row_dim=0\n",
    "        self.col_dim=1\n",
    "    def forward(self,token_ids,encoder_values):\n",
    "        word_embeddings=self.we(token_ids)\n",
    "        postional_encoding=self.pe(word_embeddings)\n",
    "        # Now we need to mask the self attention values so that when we are training\n",
    "        # model cannot look ahead and see tokens to be predicted and cheat\n",
    "        # We do this by creating a matrix mask where lower triangle is filled with 0 and everything else with values other than 0\n",
    "        # Then we replace the 0s above diagonal, which represent values we want to be masked out with True and replace 1s in lower triangle\n",
    "        # which represent the words we want to include to calculate self attention for a specific word in output with false\n",
    "        # mask=torch.tril((token_ids.size(dim=self.row_dim),token_ids.size(dim=self.row_dim)))\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=self.row_dim), token_ids.size(dim=self.row_dim))))\n",
    "\n",
    "        mask=mask==0\n",
    "        self_attention_values=self.self_attention(postional_encoding,postional_encoding,postional_encoding,mask=mask)\n",
    "        residual_connection_values=self_attention_values+postional_encoding\n",
    "        enc_dec_attention=self.enc_dec_attention(residual_connection_values,encoder_values,encoder_values)\n",
    "        residual_connection_values=enc_dec_attention+residual_connection_values\n",
    "        fc_layer_output=self.fc_layer(residual_connection_values)\n",
    "        return fc_layer_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b11792",
   "metadata": {},
   "source": [
    "Now that we have coded up the Encoder() and Decoder() classes, all that's left is to code up a Transformer() that connects the two.\n",
    "\n",
    "\n",
    "The Transformer Class\n",
    "The Transformer() class simply connects the outputs from the Encoder to the Decoder, as seen in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "716175ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "class Transformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, output_size, d_model=2, max_len=3):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            num_tokens=len(input_vocab), d_model=d_model, max_len=max_len\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            num_tokens=len(output_vocab), d_model=d_model, max_len=max_len\n",
    "        )\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "\n",
    "        encoder_values = self.encoder(inputs)\n",
    "        output_presoftmax = self.decoder(labels, encoder_values)\n",
    "\n",
    "        return output_presoftmax\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        \n",
    "        input_i, label_i = batch # collect input\n",
    "        \n",
    "        ## First, let's append the <SOS> token to tokens used as input to the Encoder...\n",
    "        input_tokens = torch.cat((torch.tensor([0]), input_i[0]))\n",
    "        \n",
    "        ## ...and to the tokens used as input to the decoder.\n",
    "        teacher_forcing = torch.cat((torch.tensor([0]), label_i[0]))\n",
    "        \n",
    "        ## Now let's add the <EOS> token to the end of the known output\n",
    "        expected_output = torch.cat((label_i[0], torch.tensor([4])))\n",
    "                \n",
    "        output_i = self.forward(input_tokens, teacher_forcing)\n",
    "        loss = self.loss(output_i, expected_output)\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c3165",
   "metadata": {},
   "source": [
    "Test Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f980e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Predicted ids tensor([0, 2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3694377/3597027536.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  div_term=1/torch.tensor(10000**(2*torch.arange(start=0, end=d_model, step=2).float()/d_model))\n"
     ]
    }
   ],
   "source": [
    "max_length=3\n",
    "transformer=Transformer(len(input_vocab),output_size=len(output_vocab),max_len=max_len)\n",
    "encoder_values=transformer.encoder(torch.tensor([0,1,3])) # <SOS> let's go\n",
    "# Then we initialize decoder with <EOS> -> [0] from decoder\n",
    "predicted_ids=torch.tensor([0])\n",
    "for i in range(max_length):\n",
    "    prediction=transformer.decoder(predicted_ids,encoder_values)\n",
    "    predicted_id=torch.tensor([torch.argmax(prediction[-1,:])])\n",
    "    predicted_ids=torch.cat([predicted_ids,predicted_id])\n",
    "    if (predicted_id==4): # if EOS token we are done\n",
    "        break\n",
    "print(\"\\n Predicted ids\",predicted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f8009",
   "metadata": {},
   "source": [
    "Train the Transformer!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "42d64dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/tmp/ipykernel_3694377/3597027536.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  div_term=1/torch.tensor(10000**(2*torch.arange(start=0, end=d_model, step=2).float()/d_model))\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(len(input_vocab), len(output_vocab), d_model=2, max_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "238b66f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 20     | train\n",
      "1 | decoder | Decoder          | 49     | train\n",
      "2 | loss    | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "69        Trainable params\n",
      "0         Non-trainable params\n",
      "69        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec8baf04866415f8e9dce3f931d0b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(transformer, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97a65797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predicted_ids: tensor([0, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "## First, a reminder of our input and output vocabularies...\n",
    "# input_vocab = {'<SOS>': 0, # Start\n",
    "#                'lets': 1,\n",
    "#                'to': 2,\n",
    "#                'go': 3}\n",
    "\n",
    "# output_vocab = {'<SOS>': 0, # Start\n",
    "#                 'ir': 1,\n",
    "#                 'vamos': 2,\n",
    "#                 'y': 3,\n",
    "#                 '<EOS>': 4} # End\n",
    "\n",
    "max_length = 3\n",
    "row_dim = 0\n",
    "col_dim = 1\n",
    "\n",
    "## Encode the user input...\n",
    "encoder_values = transformer.encoder(\n",
    "    torch.tensor([0, 1, 3])\n",
    ")  # <SOS> let's go # Expecting: 0, 2, 4 = <SOS> vamos <EOS>\n",
    "# encoder_values = transformer.encoder(torch.tensor([0, 2, 3])) # <SOS> to go  # Expecting: 0, 1, 4 = <SOS> ir <EOS>\n",
    "\n",
    "## Since we initialize the decoder with the <SOS> token, we\n",
    "## can consider that <SOS> to be the first predicted token\n",
    "predicted_ids = torch.tensor(\n",
    "    [0]\n",
    ")  # set the first predicted token to <SOS> to initialize the decoder\n",
    "for i in range(max_length):\n",
    "    ## given the current predicted tokens and the encoded input,\n",
    "    ## predict the next token with the decoder\n",
    "    ## NOTE: \"prediction\" is the output from the fully connected layer,\n",
    "    ##      not a softmax() function. We could, if we wanted to,\n",
    "    ##      Run \"prediction\" through a softmax() function, but\n",
    "    ##      since we're going to select the item with the largest value\n",
    "    ##      we can just use argmax instead...\n",
    "    prediction = transformer.decoder(predicted_ids, encoder_values)\n",
    "\n",
    "    ## Use argmax() to select the id of the predicted token\n",
    "    predicted_id = torch.tensor([torch.argmax(prediction[-1, :])])\n",
    "    ## add the predicted token id to the list of predicted ids.\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "    if predicted_id == 4:  # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "\n",
    "print(\"\\npredicted_ids:\", predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1937561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "412670fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## First, check to see if lightning is installed, if not, install it.\n",
    "##\n",
    "## NOTE: If you **do** need to install something, just know that you may need to\n",
    "##       restart your session for python to find the new module(s).\n",
    "##\n",
    "##       To restart your session:\n",
    "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
    "##         \"Restart Session\" from the pulldown menu\n",
    "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
    "##         \"Restart Kernel\" from the pulldown menu\n",
    "import pip\n",
    "try:\n",
    "  __import__(\"lightning\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"lightning\"])\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ac2dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn  ## torch.nn gives us nn.Module, nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F  # This gives us the softmax() and argmax()\n",
    "from torch.optim import Adam  # This is the optimizer we will use\n",
    "\n",
    "import lightning as L  # Lightning makes it easier to write, optimize and scale our code\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    ")  # We'll store our data in DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dc76803",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, we create a dictionary that maps vocabulary tokens to id numbers...\n",
    "token_to_id = {\n",
    "    \"what\": 0,\n",
    "    \"is\": 1,\n",
    "    \"statquest\": 2,\n",
    "    \"awesome\": 3,\n",
    "    \"<EOS>\": 4,  ## <EOS> = end of sequence\n",
    "}\n",
    "## ...then we create a dictionary that maps the ids to tokens. This will help us interpret the output.\n",
    "## We use the \"map()\" function to apply the \"reversed()\" function to each tuple (i.e. ('what', 0)) stored\n",
    "## in the token_to_id dictionary. We then use dict() to make a new dictionary from the\n",
    "## reversed tuples.\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "\n",
    "## NOTE: Because we are using a Decoder-Only Transformer, the inputs contain\n",
    "##       the questions (\"what is statquest?\" and \"statquest is what?\") followed\n",
    "##       by an <EOS> token followed by the response, \"awesome\".\n",
    "##       This is because all of those tokens will be used as inputs to the Decoder-Only\n",
    "##       Transformer during Training. (See the illustration above for more details)\n",
    "## ALSO NOTE: When we train this way, it's called \"teacher forcing\".\n",
    "##       Teacher forcing helps us train the neural network faster.\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            token_to_id[\"what\"],  ## input #1: what is statquest <EOS> awesome\n",
    "            token_to_id[\"is\"],\n",
    "            token_to_id[\"statquest\"],\n",
    "            token_to_id[\"<EOS>\"],\n",
    "            token_to_id[\"awesome\"],\n",
    "        ],\n",
    "        [\n",
    "            token_to_id[\"statquest\"],  # input #2: statquest is what <EOS> awesome\n",
    "            token_to_id[\"is\"],\n",
    "            token_to_id[\"what\"],\n",
    "            token_to_id[\"<EOS>\"],\n",
    "            token_to_id[\"awesome\"],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "## NOTE: Because we are using a Decoder-Only Transformer the outputs, or\n",
    "##       the predictions, are the input questions (minus the first word) followed by\n",
    "##       <EOS> awesome <EOS>.  The first <EOS> means we're done processing the input question\n",
    "##       and the second <EOS> means we are done generating the output.\n",
    "##       See the illustration above for more details.\n",
    "labels = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            token_to_id[\"is\"],\n",
    "            token_to_id[\"statquest\"],\n",
    "            token_to_id[\"<EOS>\"],\n",
    "            token_to_id[\"awesome\"],\n",
    "            token_to_id[\"<EOS>\"],\n",
    "        ],\n",
    "        [\n",
    "            token_to_id[\"is\"],\n",
    "            token_to_id[\"what\"],\n",
    "            token_to_id[\"<EOS>\"],\n",
    "            token_to_id[\"awesome\"],\n",
    "            token_to_id[\"<EOS>\"],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Now let's package everything up into a DataLoader...\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5ccbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len,d_token):\n",
    "        super().__init__()\n",
    "        pe=torch.zeros(max_len,d_token)\n",
    "        \n",
    "        ## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "        ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "        # Calculate 1/1000^(2i/d_model) where i is the index of current token embedding value right now we have d_model =2 so we iterate from 0,1\n",
    "        # and as we have max_len of 3 meaning three unique tokens represented by 3*2 =6 unique values for 1st token Let's we will have\n",
    "        # one sin and cos based value\n",
    "        position=torch.arange(start=0,end=max_len,step=1).float().unsqueeze(1)\n",
    "        div_term_values=1/10000**((2*torch.arange(start=0,end=d_token,step=2).float()/d_token))\n",
    "        pe[:,0::2]=torch.sin(position*div_term_values)\n",
    "        pe[:,1:2]=torch.cos(position*div_term_values)\n",
    "        self.register_buffer('pe',pe)\n",
    "    def forward(self,x):\n",
    "        return x+self.pe[:x.size(0),:]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9adccad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "div_term_values=1/10000**((2*torch.arange(start=0,end=2,step=2).float()/2))\n",
    "\n",
    "print(0 / div_term_values)\n",
    "print(1 / div_term_values)\n",
    "print(2 / div_term_values)\n",
    "print(torch.zeros(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd3eaa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term = 1/torch.tensor(10000.0)**(torch.arange(start=0, end=2, step=2).float() / 2)\n",
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407b187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60163e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, max_len,d_model):\n",
    "        super().__init__()\n",
    "        self.q_w=nn.Linear(d_model,d_model)\n",
    "        self.k_w=nn.Linear(d_model,d_model)\n",
    "        self.v_w=nn.Linear(d_model,d_model)\n",
    "        self.row_dim=0\n",
    "        self.col_dim=1\n",
    "    def forward(self,encodings_q,encodings_k,encodings_v,mask=None):\n",
    "        q=self.q_w(encodings_q)\n",
    "        k=self.k_w(encodings_k)\n",
    "        v=self.v_w(encodings_v)\n",
    "        sims=torch.matmul(q,k.transpose(self.row_dim,self.col_dim))\n",
    "        scaled_sims=sims/(q.size(self.col_dim)**0.5)\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2e6f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(L.LightningModule):\n",
    "    def __init__(self, num_tokens,d_model,max_len):\n",
    "        super().__init__()\n",
    "        L.seed_everything(seed=42)\n",
    "        self.embeddings=nn.Embedding(num_tokens,d_model)\n",
    "        self.positional_encodings = PositionalEncoding(max_len=max_len, d_token=d_model)\n",
    "        self.masked_attention = Attention(max_len, d_model)\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    def forward(self,token_ids):\n",
    "        embeddings = self.embeddings(token_ids)\n",
    "        pe = self.positional_encodings(embeddings)\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0))))\n",
    "        mask=mask==0\n",
    "        attention_outpus = self.masked_attention(pe,pe,pe,mask)\n",
    "        residual_values=attention_outpus+pe\n",
    "        outputs=self.fc_layer(residual_values)\n",
    "        return outputs\n",
    "    def configure_optimizers(self):\n",
    "        ## configure_optimizers() simply passes the parameters we want to\n",
    "        ## optimize to the optimzes and sets the learning rate\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ## training_step() is called by Lightning trainer when\n",
    "        ## we want to train the model.\n",
    "        input_tokens, labels = batch # collect input\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output, labels[0])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6db5d7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((4, 4)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2b405f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t what\n",
      "\t what\n",
      "\t what\n"
     ]
    }
   ],
   "source": [
    "## First, create a model from DecoderOnlyTransformer()\n",
    "model = Decoder(num_tokens=len(token_to_id), d_model=2, max_len=6)\n",
    "\n",
    "## Now create the input for the transformer...\n",
    "model_input = torch.tensor(\n",
    "    [\n",
    "        token_to_id[\"what\"],\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"statquest\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "    ]\n",
    ")\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "## Now get get predictions from the model\n",
    "predictions = model(model_input)\n",
    "## NOTE: \"predictions\" is the output from the fully connected layer,\n",
    "##      not a softmax() function. We could, if we wanted to,\n",
    "##      Run \"predictions\" through a softmax() function, but\n",
    "##      since we're going to select the item with the largest value\n",
    "##      we can just use argmax instead...\n",
    "## ALSO NOTE: \"predictions\" is a matrix, with one row of predicted values\n",
    "##      per input token. Since we only want the prediction from the\n",
    "##      last row (the most recent prediction) we use reverse index for the\n",
    "##      row, -1.\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "## We'll store predicted_id in an array, predicted_ids, that\n",
    "## we'll add to each time we predict a new output token.\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "## Now use a loop to predict output tokens until we get an\n",
    "## <EOS> token.\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length):\n",
    "    if (\n",
    "        predicted_id == token_to_id[\"<EOS>\"]\n",
    "    ):  # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "## Now printout the predicted output phrase.\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38d453bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfad2220e5646e6996e044cfcc63507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfc3ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "model_input = torch.tensor(\n",
    "    [\n",
    "        token_to_id[\"what\"],\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"statquest\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "    ]\n",
    ")\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "for i in range(input_length, max_length):\n",
    "    if (\n",
    "        predicted_id == token_to_id[\"<EOS>\"]\n",
    "    ):  # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71002740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "## Now let's ask the other question...\n",
    "model_input = torch.tensor(\n",
    "    [\n",
    "        token_to_id[\"statquest\"],\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"what\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "    ]\n",
    ")\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "for i in range(input_length, max_length):\n",
    "    if (\n",
    "        predicted_id == token_to_id[\"<EOS>\"]\n",
    "    ):  # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a99f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

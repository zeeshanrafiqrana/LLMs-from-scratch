{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2477aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## First, check to see if lightning is installed, if not, install it.\n",
    "##\n",
    "## NOTE: If you **do** need to install something, just know that you may need to\n",
    "##       restart your session for python to find the new module(s).\n",
    "##\n",
    "##       To restart your session:\n",
    "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
    "##         \"Restart Session\" from the pulldown menu\n",
    "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
    "##         \"Restart Kernel\" from the pulldown menu\n",
    "import pip\n",
    "try:\n",
    "  __import__(\"lightning\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"lightning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424c2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28bb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a53b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # torch will allow us to create tensors.\n",
    "import torch.nn as nn  # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F  # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import (\n",
    "    Adam,\n",
    ")  # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L  # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    ")  # these are needed for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ad1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(L.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        mean=torch.tensor(0.0)\n",
    "        std=torch.tensor(1.0)\n",
    "        # Scaling Long Term Memory\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        # Potential Long term memory to remember\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        # New Long term memory\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        # New short term memory and output\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "    def lstm_unit(self,input_value,long_term,short_term):\n",
    "        long_term_percent=torch.sigmoid((input_value*self.wlr2)+(short_term*self.wlr1)+self.blr1)\n",
    "        potential_remember_percent=torch.sigmoid((input_value*self.wpr2)+(short_term*self.wpr1)+self.bpr1)\n",
    "        potential_memory=torch.tanh((input_value*self.wo2)+(short_term*self.wo1)+self.bp1)\n",
    "        updated_long_memory=((long_term*long_term_percent)+(potential_remember_percent*potential_memory))\n",
    "        output_percent=torch.sigmoid((input_value*self.wo2)+(short_term*self.wo1)+self.bo1)\n",
    "        updated_short_memory=torch.tanh(updated_long_memory)*output_percent\n",
    "        return (updated_long_memory,updated_short_memory)\n",
    "    def forward(self, input):\n",
    "        # unroll lstm for training data for each data point/ or each past day etc\n",
    "        # Each unit will output the long term and short term  and at the end the final short term will be output\n",
    "        long_term=0\n",
    "        short_term=0\n",
    "        day1=input[0]\n",
    "        day2=input[1]\n",
    "        day3=input[2]\n",
    "        day4=input[3]\n",
    "\n",
    "        long_term, short_term = self.lstm_unit(day1, long_term, short_term)\n",
    "        long_term, short_term = self.lstm_unit(day2, long_term, short_term)\n",
    "        long_term, short_term = self.lstm_unit(day3, long_term, short_term)\n",
    "        long_term, short_term = self.lstm_unit(day4, long_term, short_term)\n",
    "        return short_term\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters())\n",
    "    def training_step(self, batch,batch_idx):\n",
    "        input_i,label_i=batch\n",
    "        output_i=self.forward(input_i[0])\n",
    "        loss=(output_i-label_i)**2\n",
    "        self.log(\"Train_loss\",loss)\n",
    "        if(label_i==0):\n",
    "            self.log(\"out_0\",output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\",output_i)\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d3f361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before optimization, the parameters are...\n",
      "wlr1 tensor(0.3367)\n",
      "wlr2 tensor(0.1288)\n",
      "blr1 tensor(0.)\n",
      "wpr1 tensor(0.2345)\n",
      "wpr2 tensor(0.2303)\n",
      "bpr1 tensor(0.)\n",
      "wp1 tensor(-1.1229)\n",
      "wp2 tensor(-0.1863)\n",
      "bp1 tensor(0.)\n",
      "wo1 tensor(2.2082)\n",
      "wo2 tensor(-0.6380)\n",
      "bo1 tensor(0.)\n",
      "\n",
      " Now let's compare the observed vs predicted value\n",
      "Company A: Observed =0, Predicted = tensor(-0.1393, grad_fn=<MulBackward0>)\n",
      "Company B: Observed =1, Predicted = tensor(-0.1537, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMbyHand()\n",
    "print(\"before optimization, the parameters are...\")\n",
    "for name,param in model.named_parameters():\n",
    "    print(name,param.data)\n",
    "\n",
    "print(\"\\n Now let's compare the observed vs predicted value\")\n",
    "print(\"Company A: Observed =0, Predicted =\", model(torch.tensor([0.,0.5,0.25,1.])))\n",
    "print(\"Company B: Observed =1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e79e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0.0, 0.5, 0.25, 1.0], [1.0, 0.5, 0.25, 1.0]])\n",
    "labels = torch.tensor([0.0, 1.0])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0184e464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b44d3675d214867a4203717c18b114b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=2000\n",
    ")  # with default learning rate, 0.001 (this tiny learning rate makes learning slow)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f8cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.4153)\n",
      "Company B: Observed = 1, Predicted = tensor(0.5853)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

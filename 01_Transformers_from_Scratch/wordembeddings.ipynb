{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24698954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.optim import (\n",
    "    Adam,\n",
    ")  # optim contains many optimizers. This time we're using Adam\n",
    "from torch.distributions.uniform import (\n",
    "    Uniform,\n",
    ")  # So we can initialize our tensors with a uniform distribution\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1f434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea5bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [1.0, 0.0, 0.0, 0.0],  # one-hot-encoding for Troll 2...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # ...is\n",
    "        [0.0, 0.0, 1.0, 0.0],  # ...great\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")  # ...Gymkata\n",
    "\n",
    "labels = torch.tensor(\n",
    "    [\n",
    "        [0.0, 1.0, 0.0, 0.0],  # \"Troll 2\" is followed by \"is\"\n",
    "        [0.0, 0.0, 1.0, 0.0],  # \"is\" is followed by \"great\"\n",
    "        [\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            1.0,\n",
    "        ],  # \"great\" isn't followed by anything, but we'll pretend it was followed by \"Gymkata\"\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "    ]\n",
    ")  # \"Gymkata\", just like \"Troll 2\", is followed by \"is\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e3bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2705f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingsFromScratch(L.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        L.seed_everything(42)\n",
    "        # Param for uniform distribution\n",
    "        min_value=-0.5\n",
    "        max_value=0.5\n",
    "        # We use uniform distribution to initialize weights because Linear\n",
    "        # layers in torhc use that to initialize weights in our case unifrom distribution is U(-0.5,0.5)\n",
    "        # in case of Linear layer its U(-sqrt(h),sqrt(k)) where k=1/input_features\n",
    "        # We have 4 input features so its U(-0.5,0.5) if calculated\n",
    "        # 1---|       |----o1      then all softmax\n",
    "        # 2---|___sum|------ 02\n",
    "        # 3---|         |---03\n",
    "        # 4----|___sum|        04\n",
    "        self.input1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.input2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.input3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.input4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        ## Now we initialize the weights that come out of the hidden layer to the \"output\"\n",
    "        ## NOTE: Again, we are excluding bias terms. This time, we exclude them simply because\n",
    "        ##       we do not need them.\n",
    "        self.output1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.output2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.output3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.output4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "    def forward(self, input):\n",
    "        input=input[0]\n",
    "        input_top_hidden = (\n",
    "            (input[0] * self.input1_w1)\n",
    "            + (input[0] * self.input2_w1)\n",
    "            + (input[0] * self.input3_w1)\n",
    "            + (input[0] * self.input4_w1)\n",
    "        )\n",
    "        input_bottom_hidden = (\n",
    "            (input[0] * self.input1_w2)\n",
    "            + (input[0] * self.input2_w2)\n",
    "            + (input[0] * self.input3_w2)\n",
    "            + (input[0] * self.input4_w2)\n",
    "        )\n",
    "        output1 = (input_top_hidden * self.output1_w1) + (\n",
    "            input_bottom_hidden * self.output1_w2\n",
    "        )\n",
    "        output2 = (input_top_hidden * self.output2_w1) + (\n",
    "            input_bottom_hidden * self.output2_w2\n",
    "        )\n",
    "        output3 = (input_top_hidden * self.output3_w1) + (\n",
    "            input_bottom_hidden * self.output3_w2\n",
    "        )\n",
    "        output4 = (input_top_hidden * self.output4_w1) + (\n",
    "            input_bottom_hidden * self.output4_w2\n",
    "        )\n",
    "        output_softmax=torch.stack([output1,output2,output3,output4])\n",
    "        return (output_softmax)\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(),lr=0.1)\n",
    "    def training_step(self, batch,batch_idx):\n",
    "        input_i,label_i=batch\n",
    "        print(input_i,label_i)\n",
    "        print(\"output\")\n",
    "\n",
    "        output_i=self.forward(input_i)\n",
    "        print(output_i)\n",
    "        loss = self.loss(output_i, label_i[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed961779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "input1_w1 tensor(0.3800)\n",
      "input1_w2 tensor(0.4200)\n",
      "input2_w1 tensor(-0.1200)\n",
      "input2_w2 tensor(0.4600)\n",
      "input3_w1 tensor(-0.1100)\n",
      "input3_w2 tensor(0.1000)\n",
      "input4_w1 tensor(-0.2400)\n",
      "input4_w2 tensor(0.2900)\n",
      "output1_w1 tensor(0.4400)\n",
      "output1_w2 tensor(-0.3700)\n",
      "output2_w1 tensor(0.4300)\n",
      "output2_w2 tensor(0.0900)\n",
      "output3_w1 tensor(0.3700)\n",
      "output3_w2 tensor(0.0700)\n",
      "output4_w1 tensor(0.2400)\n",
      "output4_w2 tensor(-0.0700)\n"
     ]
    }
   ],
   "source": [
    "modelFromScratch = WordEmbeddingsFromScratch()  # create the model...\n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in modelFromScratch.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867f7627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>token</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.382269</td>\n",
       "      <td>0.415004</td>\n",
       "      <td>Troll2</td>\n",
       "      <td>input1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.117136</td>\n",
       "      <td>0.459306</td>\n",
       "      <td>is</td>\n",
       "      <td>input2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109552</td>\n",
       "      <td>0.100895</td>\n",
       "      <td>great</td>\n",
       "      <td>input3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.243428</td>\n",
       "      <td>0.293641</td>\n",
       "      <td>Gymkata</td>\n",
       "      <td>input4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w1        w2    token   input\n",
       "0  0.382269  0.415004   Troll2  input1\n",
       "1 -0.117136  0.459306       is  input2\n",
       "2 -0.109552  0.100895    great  input3\n",
       "3 -0.243428  0.293641  Gymkata  input4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"w1\": [\n",
    "        modelFromScratch.input1_w1.item(),  ## item() pulls out the tensor value as a float\n",
    "        modelFromScratch.input2_w1.item(),\n",
    "        modelFromScratch.input3_w1.item(),\n",
    "        modelFromScratch.input4_w1.item(),\n",
    "    ],\n",
    "    \"w2\": [\n",
    "        modelFromScratch.input1_w2.item(),\n",
    "        modelFromScratch.input2_w2.item(),\n",
    "        modelFromScratch.input3_w2.item(),\n",
    "        modelFromScratch.input4_w2.item(),\n",
    "    ],\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a65066bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token that each dot represents to the graph\n",
    "# Troll 2\n",
    "plt.text(\n",
    "    df.w1[0],\n",
    "    df.w2[0],\n",
    "    df.token[0],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "# is\n",
    "plt.text(\n",
    "    df.w1[1],\n",
    "    df.w2[1],\n",
    "    df.token[1],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "# great\n",
    "plt.text(\n",
    "    df.w1[2],\n",
    "    df.w2[2],\n",
    "    df.token[2],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "# Gymkata\n",
    "plt.text(\n",
    "    df.w1[3],\n",
    "    df.w2[3],\n",
    "    df.token[3],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7c4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | loss         | CrossEntropyLoss | 0      | train\n",
      "  | other params | n/a              | 16     | n/a  \n",
      "----------------------------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdddb501cc84a96962e0b2371ab87dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-0.5042,  0.0806,  0.0535, -0.1108], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-0.7813,  0.9869,  0.1558, -0.2870], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-2.4096,  1.9296, -1.0876, -1.5673], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-3.7372,  2.8060, -2.1586, -2.6644], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-4.7241,  3.5086, -2.9869, -3.5079], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-5.4396,  4.0393, -3.6024, -4.1320], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-5.9516,  4.4286, -4.0501, -4.5845], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-6.3148,  4.7094, -4.3711, -4.9082], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-6.5705,  4.9095, -4.5989, -5.1377], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-6.7495,  5.0509, -4.7593, -5.2991], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-6.8742,  5.1502, -4.8717, -5.4121], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-6.9608,  5.2198, -4.9501, -5.4909], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.0209,  5.2685, -5.0048, -5.5458], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.0625,  5.3027, -5.0430, -5.5841], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.0915,  5.3268, -5.0697, -5.6109], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1116,  5.3439, -5.0885, -5.6298], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1259,  5.3563, -5.1020, -5.6433], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1360,  5.3655, -5.1117, -5.6531], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1434,  5.3725, -5.1190, -5.6603], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1489,  5.3780, -5.1246, -5.6659], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1531,  5.3824, -5.1291, -5.6704], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1565,  5.3863, -5.1328, -5.6740], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1593,  5.3897, -5.1360, -5.6772], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1618,  5.3928, -5.1389, -5.6801], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1640,  5.3957, -5.1416, -5.6827], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1660,  5.3985, -5.1441, -5.6853], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1680,  5.4013, -5.1466, -5.6877], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1699,  5.4041, -5.1490, -5.6901], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1718,  5.4068, -5.1515, -5.6925], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1737,  5.4096, -5.1539, -5.6949], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1756,  5.4123, -5.1563, -5.6972], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1774,  5.4151, -5.1587, -5.6997], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1793,  5.4179, -5.1612, -5.7021], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1812,  5.4207, -5.1637, -5.7045], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1832,  5.4236, -5.1662, -5.7070], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1851,  5.4265, -5.1687, -5.7095], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1871,  5.4294, -5.1713, -5.7120], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1890,  5.4324, -5.1739, -5.7145], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1910,  5.4353, -5.1765, -5.7171], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1931,  5.4383, -5.1791, -5.7197], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1951,  5.4414, -5.1818, -5.7223], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1971,  5.4444, -5.1844, -5.7249], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.1992,  5.4475, -5.1871, -5.7276], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2013,  5.4506, -5.1898, -5.7303], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2034,  5.4537, -5.1926, -5.7329], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2055,  5.4569, -5.1953, -5.7357], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2076,  5.4600, -5.1981, -5.7384], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2097,  5.4632, -5.2009, -5.7411], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2119,  5.4664, -5.2037, -5.7439], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2141,  5.4696, -5.2065, -5.7467], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2162,  5.4729, -5.2094, -5.7495], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2184,  5.4762, -5.2122, -5.7523], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2206,  5.4794, -5.2151, -5.7551], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2228,  5.4827, -5.2180, -5.7579], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2251,  5.4860, -5.2209, -5.7608], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2273,  5.4893, -5.2238, -5.7637], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2295,  5.4927, -5.2267, -5.7665], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2318,  5.4960, -5.2297, -5.7694], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2341,  5.4994, -5.2326, -5.7723], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2363,  5.5028, -5.2356, -5.7752], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2386,  5.5062, -5.2385, -5.7782], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2409,  5.5096, -5.2415, -5.7811], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2432,  5.5130, -5.2445, -5.7840], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2455,  5.5164, -5.2475, -5.7870], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2478,  5.5198, -5.2505, -5.7900], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2501,  5.5233, -5.2536, -5.7929], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2525,  5.5267, -5.2566, -5.7959], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2548,  5.5302, -5.2596, -5.7989], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2571,  5.5336, -5.2627, -5.8019], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2595,  5.5371, -5.2657, -5.8049], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2618,  5.5406, -5.2688, -5.8079], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2642,  5.5441, -5.2719, -5.8109], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2665,  5.5476, -5.2750, -5.8140], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2689,  5.5511, -5.2780, -5.8170], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2713,  5.5546, -5.2811, -5.8200], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2736,  5.5581, -5.2842, -5.8231], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2760,  5.5616, -5.2873, -5.8261], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2784,  5.5652, -5.2904, -5.8292], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2808,  5.5687, -5.2935, -5.8322], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2832,  5.5722, -5.2966, -5.8353], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2856,  5.5758, -5.2997, -5.8383], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2880,  5.5793, -5.3029, -5.8414], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2904,  5.5829, -5.3060, -5.8445], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2928,  5.5864, -5.3091, -5.8475], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2952,  5.5900, -5.3122, -5.8506], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.2976,  5.5936, -5.3154, -5.8537], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3000,  5.5971, -5.3185, -5.8568], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3024,  5.6007, -5.3217, -5.8599], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3048,  5.6042, -5.3248, -5.8629], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3072,  5.6078, -5.3279, -5.8660], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3096,  5.6114, -5.3311, -5.8691], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3120,  5.6149, -5.3342, -5.8722], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3144,  5.6185, -5.3374, -5.8753], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3168,  5.6221, -5.3405, -5.8784], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3193,  5.6256, -5.3436, -5.8815], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3217,  5.6292, -5.3468, -5.8846], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3241,  5.6328, -5.3499, -5.8877], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3265,  5.6363, -5.3531, -5.8908], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3289,  5.6399, -5.3562, -5.8938], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[1., 0., 0., 0.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([-7.3314,  5.6435, -5.3594, -5.8969], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 1., 0., 0.]]) tensor([[0., 0., 1., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 1., 0.]]) tensor([[0., 0., 0., 1.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n",
      "tensor([[0., 0., 0., 1.]]) tensor([[0., 1., 0., 0.]])\n",
      "output\n",
      "tensor([0., 0., 0., 0.], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(modelFromScratch, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3955bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>token</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257723</td>\n",
       "      <td>1.467693</td>\n",
       "      <td>Troll2</td>\n",
       "      <td>input1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.241683</td>\n",
       "      <td>1.511995</td>\n",
       "      <td>is</td>\n",
       "      <td>input2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.234098</td>\n",
       "      <td>1.153584</td>\n",
       "      <td>great</td>\n",
       "      <td>input3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.367974</td>\n",
       "      <td>1.346331</td>\n",
       "      <td>Gymkata</td>\n",
       "      <td>input4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w1        w2    token   input\n",
       "0  0.257723  1.467693   Troll2  input1\n",
       "1 -0.241683  1.511995       is  input2\n",
       "2 -0.234098  1.153584    great  input3\n",
       "3 -0.367974  1.346331  Gymkata  input4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"w1\": [\n",
    "        modelFromScratch.input1_w1.item(),  ## item() pulls out the tensor value as a float\n",
    "        modelFromScratch.input2_w1.item(),\n",
    "        modelFromScratch.input3_w1.item(),\n",
    "        modelFromScratch.input4_w1.item(),\n",
    "    ],\n",
    "    \"w2\": [\n",
    "        modelFromScratch.input1_w2.item(),\n",
    "        modelFromScratch.input2_w2.item(),\n",
    "        modelFromScratch.input3_w2.item(),\n",
    "        modelFromScratch.input4_w2.item(),\n",
    "    ],\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c953b148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## NOTE: For Troll2 and and Gymkata, we're adding offsets to where to print the tokens because otherwise\n",
    "## they will be so close to each other that they will overlap and be unreadable.\n",
    "\n",
    "## Troll 2\n",
    "plt.text(\n",
    "    df.w1[0] - 0.2,\n",
    "    df.w2[0] + 0.1,\n",
    "    df.token[0],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "## is\n",
    "plt.text(\n",
    "    df.w1[1],\n",
    "    df.w2[1],\n",
    "    df.token[1],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "## great\n",
    "plt.text(\n",
    "    df.w1[2],\n",
    "    df.w2[2],\n",
    "    df.token[2],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "## gymkata\n",
    "plt.text(\n",
    "    df.w1[3] - 0.3,\n",
    "    df.w2[3] - 0.3,\n",
    "    df.token[3],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c169f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0.], grad_fn=<RoundBackward1>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<RoundBackward1>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<RoundBackward1>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<RoundBackward1>)\n"
     ]
    }
   ],
   "source": [
    "## Let's see what the model predicts\n",
    "\n",
    "## First, let's create a softmax object...\n",
    "softmax = nn.Softmax(\n",
    "    dim=0\n",
    ")  ## dim=0 applies softmax to rows, dim=1 applies softmax to columns\n",
    "\n",
    "## Now let's...\n",
    "\n",
    "## print the predictions for \"Troll2\"\n",
    "print(\n",
    "    torch.round(\n",
    "        softmax(modelFromScratch(torch.tensor([[1.0, 0.0, 0.0, 0.0]]))), decimals=2\n",
    "    )\n",
    ")\n",
    "\n",
    "## print the predictions for \"is\"\n",
    "print(\n",
    "    torch.round(\n",
    "        softmax(modelFromScratch(torch.tensor([[0.0, 1.0, 0.0, 0.0]]))), decimals=2\n",
    "    )\n",
    ")\n",
    "\n",
    "## print the predictions for \"great\"\n",
    "print(\n",
    "    torch.round(\n",
    "        softmax(modelFromScratch(torch.tensor([[0.0, 0.0, 1.0, 0.0]]))), decimals=2\n",
    "    )\n",
    ")\n",
    "\n",
    "## print the predictions for \"Gymkata\"\n",
    "print(\n",
    "    torch.round(\n",
    "        softmax(modelFromScratch(torch.tensor([[0.0, 0.0, 0.0, 1.0]]))), decimals=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57c9fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingWithLinear(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        ## The first thing we do is set the seed for the random number generorator.\n",
    "        ## This ensures that when someone creates a model from this class, that model\n",
    "        ## will start off with the exact same random numbers as I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        ## In order to initialize weights from the 4 inputs (one for each unique word)\n",
    "        ##       to the 2 nodes in the hidden layer (top and bottom nodes), we simply make\n",
    "        ##       one call to nn.Linear() where in_features specifies the number of\n",
    "        ##       inputs and out_features specifies the number of nodes we\n",
    "        ##       are connecting them to. Since we don't want to use bias terms,\n",
    "        ##       we set bias=False\n",
    "        self.input_to_hidden = nn.Linear(in_features=4, out_features=2, bias=False)\n",
    "        ## Now, in order to connect the 2 nodes in the hidden layer to the 4 outputs, we\n",
    "        ##       make one call to nn.Linear(), where in_features specifies the number of\n",
    "        ##       nodes in hidden layer and out_features specifies the number of output values we want.\n",
    "        ##       And again, we can set bias=False\n",
    "        self.hidden_to_output = nn.Linear(in_features=2, out_features=4, bias=False)\n",
    "\n",
    "        ## We'll use CrossEntropyLoss in training_step()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        ## Unlike before, where we did all the math by hand, now we can\n",
    "        ## simply pass the input values to the weights we created with nn.Linear()\n",
    "        ## between the input and the hidden layer and save the result in \"hidden\"\n",
    "        ##\n",
    "        ## NOTE: Unlike before, we don't need to strip off the extra brackets from the\n",
    "        ##       input. the Linear ojbect knows what to do.\n",
    "        hidden = self.input_to_hidden(input)\n",
    "\n",
    "        ## Then we pass \"hidden\" to the weights we created with nn.Linear()\n",
    "        ## between the hidden layer and the output.\n",
    "        output_values = self.hidden_to_output(hidden)\n",
    "\n",
    "        return output_values\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # this configures the optimizer we want to use for backpropagation.\n",
    "\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # take a step during gradient descent.\n",
    "\n",
    "        input_i, label_i = batch  # collect input\n",
    "        output_i = self.forward(input_i)  # run input through the neural network\n",
    "        loss = self.loss(output_i, label_i)  ## loss = cross entropy\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b65115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "input_to_hidden.weight tensor([[ 0.3800,  0.4200, -0.1200,  0.4600],\n",
      "        [-0.1100,  0.1000, -0.2400,  0.2900]])\n",
      "hidden_to_output.weight tensor([[ 0.6200, -0.5200],\n",
      "        [ 0.6100,  0.1300],\n",
      "        [ 0.5200,  0.1000],\n",
      "        [ 0.3400, -0.1000]])\n"
     ]
    }
   ],
   "source": [
    "modelLinear = WordEmbeddingWithLinear()\n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in modelLinear.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d70e22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>token</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.382269</td>\n",
       "      <td>-0.109552</td>\n",
       "      <td>Troll2</td>\n",
       "      <td>input1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.415004</td>\n",
       "      <td>0.100895</td>\n",
       "      <td>is</td>\n",
       "      <td>input2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.117136</td>\n",
       "      <td>-0.243428</td>\n",
       "      <td>great</td>\n",
       "      <td>input3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.459306</td>\n",
       "      <td>0.293641</td>\n",
       "      <td>Gymkata</td>\n",
       "      <td>input4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w1        w2    token   input\n",
       "0  0.382269 -0.109552   Troll2  input1\n",
       "1  0.415004  0.100895       is  input2\n",
       "2 -0.117136 -0.243428    great  input3\n",
       "3  0.459306  0.293641  Gymkata  input4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    ## NOTE: Unlike before, when we called item() on each individual\n",
    "    ##       Weight, now that we are using nn.Linear, we access the\n",
    "    ##       Weights with \".weight\". We then have to remove the gradients\n",
    "    ##       associated with each Weight, so we also call .detach().\n",
    "    ##       Lastly, we then convert the tensor to a numpy array with\n",
    "    ##       numpy().\n",
    "    \"w1\": modelLinear.input_to_hidden.weight.detach()[\n",
    "        0\n",
    "    ].numpy(),  # [0] = Weights to top activation function\n",
    "    \"w2\": modelLinear.input_to_hidden.weight.detach()[\n",
    "        1\n",
    "    ].numpy(),  # [1] = Weights to bottom activation function\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token each dot represents to the graph\n",
    "\n",
    "## Troll 2\n",
    "plt.text(\n",
    "    df.w1[0],\n",
    "    df.w2[0],\n",
    "    df.token[0],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "## is\n",
    "plt.text(\n",
    "    df.w1[1],\n",
    "    df.w2[1],\n",
    "    df.token[1],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "## great\n",
    "plt.text(\n",
    "    df.w1[2],\n",
    "    df.w2[2],\n",
    "    df.token[2],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "## Gymkata\n",
    "plt.text(\n",
    "    df.w1[3],\n",
    "    df.w2[3],\n",
    "    df.token[3],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cb70deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | input_to_hidden  | Linear           | 8      | train\n",
      "1 | hidden_to_output | Linear           | 8      | train\n",
      "2 | loss             | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/hadi/Documents/statquest/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68b52999139443d9fa99de2a4a4e1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(modelLinear, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d87ed896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>token</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.510447</td>\n",
       "      <td>-0.068821</td>\n",
       "      <td>Troll2</td>\n",
       "      <td>input1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.017650</td>\n",
       "      <td>2.482441</td>\n",
       "      <td>is</td>\n",
       "      <td>input2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.974159</td>\n",
       "      <td>-1.818951</td>\n",
       "      <td>great</td>\n",
       "      <td>input3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.435523</td>\n",
       "      <td>-0.222807</td>\n",
       "      <td>Gymkata</td>\n",
       "      <td>input4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w1        w2    token   input\n",
       "0  2.510447 -0.068821   Troll2  input1\n",
       "1 -1.017650  2.482441       is  input2\n",
       "2 -1.974159 -1.818951    great  input3\n",
       "3  2.435523 -0.222807  Gymkata  input4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"w1\": modelLinear.input_to_hidden.weight.detach()[0].numpy(),\n",
    "    \"w2\": modelLinear.input_to_hidden.weight.detach()[1].numpy(),\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fec2382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token each dot represents to the graph\n",
    "## NOTE: For Troll2 and and Gymkata, we're adding offsets to where to print the tokens because otherwise\n",
    "## they will be so close to each other that they will overlap and be unreadable.\n",
    "\n",
    "# Troll 2\n",
    "plt.text(\n",
    "    df.w1[0] - 0.2,\n",
    "    df.w2[0] - 0.3,\n",
    "    df.token[0],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "# is\n",
    "plt.text(\n",
    "    df.w1[1],\n",
    "    df.w2[1],\n",
    "    df.token[1],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "# great\n",
    "plt.text(\n",
    "    df.w1[2],\n",
    "    df.w2[2],\n",
    "    df.token[2],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "# Gymkata\n",
    "plt.text(\n",
    "    df.w1[3] - 0.3,\n",
    "    df.w2[3] + 0.2,\n",
    "    df.token[3],\n",
    "    horizontalalignment=\"left\",\n",
    "    size=\"medium\",\n",
    "    color=\"black\",\n",
    "    weight=\"semibold\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "400a752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0.]], grad_fn=<RoundBackward1>)\n",
      "tensor([[0., 0., 1., 0.]], grad_fn=<RoundBackward1>)\n",
      "tensor([[0., 0., 0., 1.]], grad_fn=<RoundBackward1>)\n",
      "tensor([[0., 1., 0., 0.]], grad_fn=<RoundBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the model predicts\n",
    "softmax = nn.Softmax(\n",
    "    dim=1\n",
    ")  ## dim=0 applies softmax to rows, dim=1 applies softmax to columns\n",
    "\n",
    "print(\n",
    "    torch.round(softmax(modelLinear(torch.tensor([[1.0, 0.0, 0.0, 0.0]]))), decimals=2)\n",
    ")  ## print the predictions for \"Troll2\"\n",
    "print(\n",
    "    torch.round(softmax(modelLinear(torch.tensor([[0.0, 1.0, 0.0, 0.0]]))), decimals=2)\n",
    ")  ## print the predictions for \"is\"\n",
    "print(\n",
    "    torch.round(softmax(modelLinear(torch.tensor([[0.0, 0.0, 1.0, 0.0]]))), decimals=2)\n",
    ")  ## print the predictions for \"great\"\n",
    "print(\n",
    "    torch.round(softmax(modelLinear(torch.tensor([[0.0, 0.0, 0.0, 1.0]]))), decimals=2)\n",
    ")  ## print the predictions for \"Gymkata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fefdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7874bee",
   "metadata": {},
   "source": [
    "Use nn.Embedding() to load and use pre-trained Word Embeddings\n",
    "Now that we have created embeddings for each token in the vocabulary, we can store them in an nn.Embedding() object so that we can access them with the tokens, rather than the one-hot-encoded versions of the tokens. This makes them easily portable to other applications.\n",
    "\n",
    "First, let's just print out the embedding values that we created in modelLinear and that we want to add to an nn.Embedding() object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89acaa2",
   "metadata": {},
   "source": [
    "modelLinear.input_to_hidden.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7ea09",
   "metadata": {},
   "source": [
    "##  NOTE: We have to transpose the original embedding values (from w1 and w2) for nn.Embedding()\n",
    "##        and we do this with adding a '.T' to modelLinear.input_to_hidden.weight.T\n",
    "word_embeddings = nn.Embedding.from_pretrained(modelLinear.input_to_hidden.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672e7d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a0ae649",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  NOTE: We have to transpose the original embedding values (from w1 and w2) for nn.Embedding()\n",
    "##        and we do this with adding a '.T' to modelLinear.input_to_hidden.weight.T\n",
    "word_embeddings = nn.Embedding.from_pretrained(modelLinear.input_to_hidden.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7595260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.5104, -0.0688],\n",
       "        [-1.0177,  2.4824],\n",
       "        [-1.9742, -1.8190],\n",
       "        [ 2.4355, -0.2228]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83173314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5104, -0.0688])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "491b56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"Troll2\": 0, \"is\": 1, \"great\": 2, \"Gymkata\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f44d9ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5104, -0.0688])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings(torch.tensor(vocab[\"Troll2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64a02422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.4355, -0.2228])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings(torch.tensor(vocab['Gymkata']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb17b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
